# Cloud Data Pipeline PoC

This proof-of-concept demonstrates a simple AWS-based data pipeline that automates the ingestion of CSV files into a DynamoDB table using S3 and Lambda.

## ğŸ“¦ Architecture Overview

1. A CSV file is uploaded to an S3 bucket.
2. This triggers a Lambda function using S3 event notifications.
3. The Lambda function parses the file and inserts the records into DynamoDB.

## ğŸš€ Technologies Used

- AWS S3
- AWS Lambda (Python 3.9)
- AWS DynamoDB
- Terraform for IaC (Infrastructure as Code)

## âš™ï¸ Trigger Points

- S3 `ObjectCreated` event triggers the Lambda
- Lambda processes CSV rows and writes to DynamoDB

## ğŸ“ˆ Monitoring & Logging

- CloudWatch Logs integrated with Lambda
- Basic error handling and row-level validation included
- Terraform state stored locally for PoC (can be updated for remote backend)

## ğŸ› ï¸ Getting Started

1. Deploy infrastructure with Terraform:
```bash
cd terraform
terraform init
terraform apply
```

2. Upload `sample_data.csv` to the S3 bucket created by Terraform

3. Monitor Lambda execution and DynamoDB for processed records

## ğŸ‘¤ Author

Created by Shatrugna Vad â€“ Enterprise Architect  
Cloud | Data | Automation | AWS | Platform Strategy  
[LinkedIn](https://www.linkedin.com/in/your-link)

## ğŸ“œ License

This content is provided for educational and demonstration purposes under the MIT License.
